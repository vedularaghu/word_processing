{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['time','value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/a1.filtered.csv\",\n",
    "                  names=cols, header=0) # ON\n",
    "df2 = pd.read_csv(\"./data/a2.filtered.csv\",\n",
    "                 names=cols, header=0) # OFF\n",
    "df3 = pd.read_csv(\"./data/a3.filtered.csv\",\n",
    "                 names=cols, header=0) # RIGHT\n",
    "df4 = pd.read_csv(\"./data/a4.filtered.csv\",\n",
    "                 names=cols, header=0) # LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing dependencies..\n"
     ]
    }
   ],
   "source": [
    "print('importing dependencies..')\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing dependencies..\n"
     ]
    }
   ],
   "source": [
    "print('importing dependencies..')\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Score(object):\n",
    "    \n",
    "    def __init__(self, root = \"./data/\", steps=3, shuffle=True, scoring='accuracy', attr='filtered', seed=101):        \n",
    "        self.models = [\n",
    "                        ('log_r', LogisticRegressionCV()),\n",
    "                        ('dt', DecisionTreeClassifier()),\n",
    "                        ('rf', RandomForestClassifier()),\n",
    "                        ('nb', GaussianNB()),\n",
    "                        ('knn', KNeighborsClassifier()),\n",
    "                        ('lda', LinearDiscriminantAnalysis())\n",
    "                    ]\n",
    "        self._seed_ = seed\n",
    "        self._root_ = root\n",
    "        self._attr_= attr\n",
    "        self._shuffle_ = shuffle\n",
    "        self._scoring_ = scoring\n",
    "        self._steps_ = steps\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "    \n",
    "    def create_data_feature_steps(self, data, steps=3):\n",
    "        '''\n",
    "        Creating steps feature from data file\n",
    "        '''\n",
    "        new_df = []\n",
    "        for i in range(steps, data.shape[0]):\n",
    "            new_df = np.append(new_df,\n",
    "                               data[i-steps: steps+i])\n",
    "        new_df_cols = ['col'+str(i) for i in range(steps)]\n",
    "        new_df = pd.DataFrame(new_df.reshape(-1,steps),\n",
    "                     columns=new_df_cols)\n",
    "        return new_df\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        '''\n",
    "        Dropping the 'time' column, Scaling and creating features steps \n",
    "        '''\n",
    "        data = data.drop('time', axis=1)\n",
    "        data = StandardScaler().fit_transform(data)\n",
    "        data = create_data_feature_steps(data, steps=_steps_)\n",
    "        return data\n",
    "    \n",
    "    def read_multiple_files(path=root):\n",
    "        '''\n",
    "        Reading Multiple files and creating the final dataset.\n",
    "        '''\n",
    "\n",
    "        cols = ['time','value']\n",
    "\n",
    "        all_files = os.listdir(path)\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        for f in tqdm(all_files):\n",
    "            if f.split('.')[1]==self._attr_:\n",
    "\n",
    "                # reading csv file\n",
    "                df = pd.read_csv(os.path.join(path, f),\n",
    "                                 names=cols, header=0)\n",
    "\n",
    "                # preprocessing data with steps\n",
    "                df = preprocess_data(df, 3)\n",
    "\n",
    "                # adding labels\n",
    "                labels = np.array([list(\n",
    "                                    f.split('.')[0]\n",
    "                                    )[1]\n",
    "                               ]*df.shape[0]).astype(int)\n",
    "\n",
    "                df['label'] = labels\n",
    "\n",
    "                # appending df to main data\n",
    "                data = data.append(df)\n",
    "\n",
    "        # shuffling\n",
    "        if self._shuffle_:\n",
    "            data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        return data.drop('label', axis=1), data['label']\n",
    "    \n",
    "\n",
    "    \n",
    "    def run_read_files(self):\n",
    "        print('Reading files..')\n",
    "        self.X, self.y = read_multiple_files()\n",
    "\n",
    "    def plot_base_model_score(self):\n",
    "            \n",
    "        def get_model_score(X, y, estimator, name):\n",
    "            print('*'*15, name, '*'*15)\n",
    "\n",
    "            print('Init folds...')\n",
    "            # fold\n",
    "            kfold = KFold(10, True, random_state=self._seed_)\n",
    "\n",
    "            print('Training...')\n",
    "            model_score = cross_val_score(estimator, X, y,\n",
    "                            cv=kfold,\n",
    "                            verbose=1,\n",
    "                            scoring=self._scoring_)\n",
    "\n",
    "            print('Evaluation:')\n",
    "\n",
    "            result = 'MEAN: %.3f \\nSTD: (%.3f)' %(model_score.mean(), model_score.std())\n",
    "            print(result)\n",
    "            print( '\\n')\n",
    "\n",
    "\n",
    "            return model_score.mean()\n",
    "\n",
    "        scores = []\n",
    "        names = []\n",
    "        for name, model in self.models:\n",
    "            scores.append(get_model_score(self.X, self.y, model, name))\n",
    "            names.append(name)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(np.array(range(len(scores))), scores)\n",
    "        plt.xticks(np.array(range(len(scores))), names)\n",
    "        plt.show()\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = Base_Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path='nb_01.pkl', pred_file):\n",
    "    import pickle\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    model_pkl = open(path, 'rb')\n",
    "    saved_model = pickle.load(model_pkl)\n",
    "    print(\"Loading model :: \", saved_model)\n",
    "    saved_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_feature_steps(data, steps=3):\n",
    "    new_df = []\n",
    "    for i in range(steps, data.shape[0]):\n",
    "        new_df = np.append(new_df,\n",
    "                           data[i-steps: steps+i])\n",
    "    new_df_cols = ['col'+str(i) for i in range(steps)]\n",
    "    new_df = pd.DataFrame(new_df.reshape(-1,steps),\n",
    "                 columns=new_df_cols)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, steps=3):\n",
    "    data = data.drop('time', axis=1)\n",
    "    data = StandardScaler().fit_transform(data)\n",
    "    data = create_data_feature_steps(data, steps=steps)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multiple_files(path=root, attr='filtered', shuffle=True):\n",
    "    \n",
    "    cols = ['time','value']\n",
    "    \n",
    "    all_files = os.listdir(path)\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for f in all_files:\n",
    "        if f.split('.')[1]==attr:\n",
    "            \n",
    "            # reading csv file\n",
    "            df = pd.read_csv(os.path.join(path, f),\n",
    "                             names=cols, header=0)\n",
    "            \n",
    "            # preprocessing data with steps\n",
    "            df = preprocess_data(df, 3)\n",
    "            \n",
    "            # adding labels\n",
    "            labels = np.array([list(\n",
    "                                f.split('.')[0]\n",
    "                                )[1]\n",
    "                           ]*df.shape[0]).astype(int)\n",
    "            \n",
    "            df['label'] = labels\n",
    "            \n",
    "            # appending df to main data\n",
    "            data = data.append(df)\n",
    "    \n",
    "    # shuffling\n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_multiple_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.drop('label', axis=1), data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_final = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_final.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_name = 'nb_02.pkl'\n",
    "pickle_file = open(pickle_file_name, mode='wb')\n",
    "pickle.dump(gnb_final, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ::  GaussianNB(priors=None, var_smoothing=1e-09)\n"
     ]
    }
   ],
   "source": [
    "# loading pickle file\n",
    "model_pkl = open(pickle_file_name, 'rb')\n",
    "saved_model = pickle.load(model_pkl)\n",
    "print(\"Loading model :: \", saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.sample(frac=0.3, random_state=101)\n",
    "X_test, y_test = data_test.drop('label', axis=1), data_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00      3706\n",
      "           2       0.29      1.00      0.45      4270\n",
      "           3       0.00      0.00      0.00      3431\n",
      "           4       0.00      0.00      0.00      3330\n",
      "\n",
      "   micro avg       0.29      0.29      0.29     14737\n",
      "   macro avg       0.07      0.25      0.11     14737\n",
      "weighted avg       0.08      0.29      0.13     14737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, saved_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
